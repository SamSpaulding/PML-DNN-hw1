{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nd/projects/pml-neuralnet-hw/PML-DNN-hw1/neural_net\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import personalized_neural_net as neural_net\n",
    "print(os.getcwd())\n",
    "%pylab inline\n",
    "\n",
    "reload(neural_net)\n",
    "neural_net.reload_files()\n",
    "\n",
    "dropbox_path = '/Users/nd/projects/pml-neuralnet-hw/PML-DNN-hw1/'\n",
    "filename = 'parkinsons_supervised.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is iteration 0 !!!!!\n",
      "Original data length was 5875\n",
      "After dropping rows with nan in any label column, length is 5875\n",
      "3581 rows in training data\n",
      "1167 rows in validation data\n",
      "1127 rows in testing data\n",
      "\n",
      "Performing regression.\n",
      "Input dimensions (number of features): 16\n",
      "Number of classes/outputs: 2\n",
      "\n",
      "Building computation graph...\n",
      "last non-personalized layer size was\n",
      "32\n",
      "(16, 128)\n",
      "Okay, making a neural net with the following structure:\n",
      "Non-Personalized Layers:\n",
      "[('16x128', '128'), ('128x64', '64'), ('64x32', '32')]\n",
      "Personalized Layers:\n",
      "42 parallel copies of a (32, 8) layer\n",
      " and \n",
      "42 parallel copies of a (8, 2) layer\n",
      "TF_X shape\n",
      "<unknown>\n",
      "hidden1 has shape\n",
      "(?, 128)\n",
      "hidden2 has shape\n",
      "(?, 64)\n",
      "hidden3 has shape\n",
      "(?, 32)\n",
      "returning output of hidden5!\n",
      "(?, 2)\n",
      "subject output is of shape!\n",
      "<unknown>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-496279df3866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mresults_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"x\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_baseline_HPs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-496279df3866>\u001b[0m in \u001b[0;36mtest_baseline_HPs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     net = neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n\u001b[1;32m     16\u001b[0m                                                \u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                weight_penalty, clip_gradients, model_type='regression')\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_every_nth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nd/projects/pml-neuralnet-hw/PML-DNN-hw1/neural_net/personalized_neural_net.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, model_name, layer_sizes, batch_size, learning_rate, dropout_prob, weight_penalty, clip_gradients, model_type, checkpoint_dir)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Set up tensorflow computation graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Set up and initialize tensorflow session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nd/projects/pml-neuralnet-hw/PML-DNN-hw1/neural_net/personalized_neural_net.pyc\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mtrainable_p_biases0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_biases0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mtrainable_p_biases1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_biases1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mtrainable_p_weights0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_weights0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not Tensor"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "layer_sizes = [128,32]\n",
    "batch_size = 25\n",
    "learning_rate = .001\n",
    "dropout_prob = 1.0\n",
    "weight_penalty = 0.0\n",
    "clip_gradients = False\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "def test_baseline_HPs():\n",
    "    for i in range(0,1):\n",
    "                    clip_gradients = False\n",
    "                    print('this is iteration ' + str(i) + ' !!!!!')\n",
    "                    net = neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n",
    "                                               [128,64,32], batch_size, learning_rate, dropout_prob, \n",
    "                                               weight_penalty, clip_gradients, model_type='regression')\n",
    "                    dir(net)\n",
    "                    net.train(num_steps=10, output_every_nth=10000)\n",
    "                    results_dict[(str(layer_sizes[0]) + \"x\" + str(layer_sizes[1]), learning_rate, weight_penalty, dropout_prob, i)] = net.test_on_validation()\n",
    "                    \n",
    "test_baseline_HPs()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data length was 5875\n",
      "After dropping rows with nan in any label column, length is 5875\n",
      "3581 rows in training data\n",
      "1167 rows in validation data\n",
      "1127 rows in testing data\n",
      "\n",
      "Performing regression.\n",
      "Input dimensions (number of features): 16\n",
      "Number of classes/outputs: 2\n",
      "\n",
      "Building computation graph...\n",
      "Okay, making a neural net with the following structure:\n",
      "[('16x128', '128'), ('128x32', '32'), ('32x2', '2')]\n"
     ]
    }
   ],
   "source": [
    "net = neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n",
    "                               layer_sizes, batch_size, learning_rate, dropout_prob, \n",
    "                               weight_penalty, clip_gradients, model_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find good hyperparameters for the base set\n",
    "\n",
    "layer_size_cands = [[64, 32], [128, 64, 32], [32, 32, 32], [32, 16, 8]]\n",
    "learning_rates = [0.01, .1]\n",
    "weight_penalties = [0.01, .05]\n",
    "dropouts = [0.9, .8]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_for_HPs():\n",
    "    rmse_results = []\n",
    "    dict = {}\n",
    "    import time\n",
    "    start = time.time()\n",
    "    index = 0\n",
    "    for learning_rate in learning_rates:\n",
    "        for weight_penalty in weight_penalties:\n",
    "            for dropout_prob in dropouts:\n",
    "                for i in range(0,5):\n",
    "                    clip_gradients = True\n",
    "                    print('this is iteration ' + str(i) + ' !!!!!')\n",
    "                    net = neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n",
    "                                               layer_size_cands[index], batch_size, learning_rate, dropout_prob, \n",
    "                                               weight_penalty, clip_gradients, model_type='regression')\n",
    "                    net.train(num_steps=50000, output_every_nth=10000)\n",
    "                    dict[(str(layer_size_cands[index][0]) + \"x\" + str(layer_size_cands[index][1]), learning_rate, weight_penalty, dropout_prob, i)] = net.test_on_validation()\n",
    "\n",
    "    end = time.time()\n",
    "    print end-start\n",
    "    net.plot_training_progress()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### debugging neural network using session.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.35033551, -1.78116202, -0.68470937, -1.40052152, -0.30320659,\n",
      "        -0.1525038 , -0.25575474, -0.20323569, -0.25678062,  0.96460992,\n",
      "         0.52912575,  0.97140121,  1.18462873,  0.82155502,  0.97114718,\n",
      "        -0.20225847, -0.40879247,  1.4391408 ,  0.14954013, -0.16320474],\n",
      "       [-0.62218499, -0.76638234,  1.46047366,  0.49315697, -0.65916598,\n",
      "        -0.88036144, -0.54216307, -0.54065609, -0.54115933, -0.91260463,\n",
      "        -0.95570928, -0.86152756, -0.82540822, -0.99184608, -0.86126816,\n",
      "        -0.48749965,  1.05310428, -2.19696879,  0.07246424, -1.14588571],\n",
      "       [ 0.4313789 , -0.87913561,  1.46047366, -1.33126593, -0.18627307,\n",
      "        -0.4621276 , -0.04247193, -0.14307547, -0.0434966 ,  0.47931957,\n",
      "         0.38813251,  0.52619565,  0.54269242,  0.58743757,  0.52594227,\n",
      "        -0.46723726,  0.82877713, -1.87455714,  1.49147642, -0.31518951],\n",
      "       [-0.70322841,  0.92491704,  1.46047366, -0.54672247, -0.43217739,\n",
      "        -0.71460325, -0.29841128, -0.35755974, -0.2984218 , -0.50852293,\n",
      "        -0.54154164, -0.40329537, -0.48849687, -0.57709491, -0.40354744,\n",
      "        -0.30525616,  0.3612718 , -0.7216025 , -1.23093414, -1.05796969],\n",
      "       [ 0.99868256, -0.65362906, -0.68470937, -1.01195133, -0.05214344,\n",
      "         0.42438021, -0.24661404, -0.07245259, -0.2456086 , -0.21507327,\n",
      "        -0.29039744, -0.40942556, -0.23581339,  0.12677111, -0.40916678,\n",
      "        -0.1134883 , -0.28225496,  1.69729197,  0.77528948,  0.14812773],\n",
      "       [ 0.4313789 , -0.87913561,  1.46047366,  0.88773161, -0.35995373,\n",
      "        -0.61253256, -0.43552166, -0.38110068, -0.43553296, -0.43947592,\n",
      "        -0.47985709, -0.41862085, -0.38020396, -0.41513005, -0.41836208,\n",
      "        -0.43581539,  1.48903418,  0.24280785,  0.97909051, -0.36442247],\n",
      "       [-0.29801151,  0.02289071, -0.68470937,  0.43758833, -0.15703969,\n",
      "        -0.02086393, -0.00286226, -0.17707907, -0.00388671,  0.13486934,\n",
      "         0.07970981,  0.29324821,  0.06078889,  0.13333043,  0.29299515,\n",
      "        -0.36134017,  0.23850451, -0.80997318, -0.2570774 , -0.34244347],\n",
      "       [-0.62218499, -0.76638234,  1.46047366, -1.4646492 , -0.27053398,\n",
      "        -0.52268761, -0.17044161, -0.28170556, -0.16943574, -0.90083522,\n",
      "        -0.93808514, -0.84390324, -0.8061561 , -0.98175478, -0.84364384,\n",
      "        -0.37314022,  0.55331659, -1.95768201, -0.15918539, -1.02950692],\n",
      "       [-0.46009827,  0.02289071, -0.68470937, -0.71697134, -0.13296513,\n",
      "         0.02007806, -0.10340987, -0.27385855, -0.10240362, -0.75567967,\n",
      "        -0.7970919 , -0.74888521, -0.71952176, -0.90354747, -0.74888134,\n",
      "        -0.25077358,  0.26041883, -0.40080491,  0.21634862,  0.55309075],\n",
      "       [-0.13592476,  0.24839731, -0.68470937,  0.18510531,  0.29693753,\n",
      "         0.58757955,  0.25003022,  0.17341962,  0.24900721,  0.58995163,\n",
      "         0.92126316,  0.63653922,  0.5884161 ,  0.31194589,  0.63679653,\n",
      "        -0.11508974, -0.2593981 ,  0.59558433,  0.60763538,  0.32044306]], dtype=float32), array([[ 25.39500046,  30.98800087],\n",
      "       [ 15.18000031,  20.49099922],\n",
      "       [  7.99539995,  11.99499989],\n",
      "       [ 16.69099998,  24.61800003],\n",
      "       [ 25.60400009,  32.30199814],\n",
      "       [ 12.        ,  17.29999924],\n",
      "       [  5.73769999,   7.        ],\n",
      "       [  7.10370016,  11.26099968],\n",
      "       [  8.38829994,  16.83499908],\n",
      "       [ 10.11499977,  16.05699921]], dtype=float32), array([[ 22.32614899,  29.73100853],\n",
      "       [ 11.60329628,  17.25259399],\n",
      "       [ 10.11791801,  17.03334236],\n",
      "       [ 21.55584145,  30.11194992],\n",
      "       [ 26.1774559 ,  35.34766388],\n",
      "       [  9.67421246,  14.78485966],\n",
      "       [ 15.9302969 ,  21.96829224],\n",
      "       [ 10.97653675,  16.11657333],\n",
      "       [ 14.63603973,  20.84116745],\n",
      "       [ 14.88921642,  21.46494293]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print net.session.run([net.tf_X,net.tf_Y, net.logits], feed_dict=feed_dict)\n",
    "#print \"X\", np.shape(tf_X), \"Y\", np.shape(tf_Y), \"logits\", np.shape(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# To see the tensorflow model you've built in tensorboard, run the following\n",
    "journalist = tf.summary.FileWriter(net.checkpoint_dir, graph=net.session.graph)\n",
    "journalist.flush()\n",
    "\n",
    "# Now enter the following command at the command line\n",
    "# > tensorboard --logdir 'saved_models'\n",
    "# (you may need to replace 'saved_models' with the path to the actual \n",
    "# net.checkpoint_dir you are using)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
